### Building a custom Neural Network Deep Learning Model

### Training
- Backpropagation (Gradient Descent using reverse-mode autodiff)

##Training Steps
- Compute output error
- Compute contirbution of each neuron in the previous - hidden layer have contributed
- Back-propogate taht error in a reverse pass
- Tweak weights to reduce the error using gradient descent


### Activation Functions
- Logistic Function
- Hyperbolic tangent function
- Exponentional liner unit (ELU)
- ReLU function (Rectified Linear Unit) --> COMMON
 
### Optimization Functions
- Momentum Optimization
- Nesterov Accelerated Gradient -> Tweak of Momentum Optimization (Computes Momentum slightly ahead of you, not where you are)
- RMSProp -> Adaptive learning rate to help point towards the minimum
- Adam -> Adaptive momentum estimation (Momentum + RMSProd combined)


### Testing


### Execution


### Results
- Gradient Descent (Loss Curves)